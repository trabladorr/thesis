
#ifndef _MULT_DIRECT_SHUFFLEMULT_INLINE
#define _MULT_DIRECT_SHUFFLEMULT_INLINE

//calculate the multiplication of 16 bytes, using two shuffle multiplications
static inline __m128i shuffle_mult_gf16(__m128i in_0123, __m128i in_4567, __m128i in_89ab, __m128i in_cdef,\
	 	__m128i mul_lower_0123, __m128i mul_upper_0123,\
	 	__m128i mul_upper_4567,\
	 	__m128i mul_lower_89ab, __m128i mul_upper_89ab,\
	 	__m128i mul_lower_cdef){

	__m128i tmp = _mm_shuffle_epi8(mul_lower_cdef,in_cdef);
	tmp = _mm_xor_si128(tmp, _mm_shuffle_epi8(mul_lower_89ab,in_89ab));
	tmp = _mm_xor_si128(tmp, _mm_shuffle_epi8(mul_upper_4567,in_4567));
	tmp = _mm_xor_si128(tmp, _mm_shuffle_epi8(mul_upper_0123,in_0123));

	tmp = _mm_xor_si128(tmp, _mm_srli_epi16(_mm_shuffle_epi8(mul_upper_89ab,in_89ab), 8));
	tmp = _mm_xor_si128(tmp, _mm_slli_epi16(_mm_shuffle_epi8(mul_lower_0123,in_0123), 8));

	return tmp;
}

//input in contains 2 vectors to mutliply
//return ret contains 2 vectors multiplied
//multiply using four shuffle multiplications
static inline __m128i gf16_anubis_mult_2_shuffle(__m128i in){
	//TODO: two input 128 registers, interleaved during multiplication to utilize full shuffle capacity

	//mask to prevent shuffle zeroing based on highest order bit, ORed to low order multiplication
	const __m128i midword_zeroes = _mm_setr_epi8(0xf0,0xf0,0xf0,0xf0,0xf0,0xf0,0xf0,0xf0,0xf0,0xf0,0xf0,0xf0,0xf0,0xf0,0xf0,0xf0);
	const __m128i shuffle_no_zeroing_mask = _mm_setr_epi8(0x0f,0x0f,0x0f,0x0f,0x0f,0x0f,0x0f,0x0f,0x0f,0x0f,0x0f,0x0f,0x0f,0x0f,0x0f,0x0f);
	const __m128i shuffle_zeroing_odd_mask = _mm_setr_epi8(0xff,0x80,0xff,0x80,0xff,0x80,0xff,0x80,0xff,0x80,0xff,0x80,0xff,0x80,0xff,0x80);	
	const __m128i shuffle_zeroing_even_mask = _mm_setr_epi8(0x80,0xff,0x80,0xff,0x80,0xff,0x80,0xff,0x80,0xff,0x80,0xff,0x80,0xff,0x80,0xff);
	
	const __m128i shuffle_x2 = _mm_setr_epi8(0x2, 0x3, 0x0, 0x1, 0x6, 0x7, 0x4, 0x5, 0xa, 0xb, 0x8, 0x9, 0xe, 0xf, 0xc, 0xd);
	const __m128i shuffle_x4 = _mm_setr_epi8(0x4, 0x5, 0x6, 0x7, 0x0, 0x1, 0x2, 0x3, 0xc, 0xd, 0xe, 0xf, 0x8, 0x9, 0xa, 0xb);
	const __m128i shuffle_x6 = _mm_setr_epi8(0x6, 0x7, 0x4, 0x5, 0x2, 0x3, 0x0, 0x1, 0xe, 0xf, 0xc, 0xd, 0xa, 0xb, 0x8, 0x9);

	const __m128i mul_x2_lower_cdef = _mm_setr_epi8( 0x0, 0x2, 0x4, 0x6, 0x8, 0xa, 0xc, 0xe, 0x10, 0x12, 0x14, 0x16, 0x18, 0x1a, 0x1c, 0x1e );
	const __m128i mul_x2_lower_89ab = _mm_setr_epi8( 0x0, 0x20, 0x40, 0x60, 0x80, 0xa0, 0xc0, 0xe0, 0x0, 0x20, 0x40, 0x60, 0x80, 0xa0, 0xc0, 0xe0 );
	const __m128i mul_x2_lower_0123 = _mm_setr_epi8( 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x2d, 0x2d, 0x2d, 0x2d, 0x2d, 0x2d, 0x2d, 0x2d );
	const __m128i mul_x2_upper_89ab = _mm_setr_epi8( 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x1, 0x1, 0x1, 0x1, 0x1, 0x1, 0x1, 0x1 );
	const __m128i mul_x2_upper_4567 = _mm_setr_epi8( 0x0, 0x2, 0x4, 0x6, 0x8, 0xa, 0xc, 0xe, 0x10, 0x12, 0x14, 0x16, 0x18, 0x1a, 0x1c, 0x1e );
	const __m128i mul_x2_upper_0123 = _mm_setr_epi8( 0x0, 0x20, 0x40, 0x60, 0x80, 0xa0, 0xc0, 0xe0, 0x0, 0x20, 0x40, 0x60, 0x80, 0xa0, 0xc0, 0xe0 );
	
	const __m128i mul_x4_lower_cdef = _mm_setr_epi8( 0x0, 0x4, 0x8, 0xc, 0x10, 0x14, 0x18, 0x1c, 0x20, 0x24, 0x28, 0x2c, 0x30, 0x34, 0x38, 0x3c );
	const __m128i mul_x4_lower_89ab = _mm_setr_epi8( 0x0, 0x40, 0x80, 0xc0, 0x0, 0x40, 0x80, 0xc0, 0x0, 0x40, 0x80, 0xc0, 0x0, 0x40, 0x80, 0xc0 );
	const __m128i mul_x4_lower_0123 = _mm_setr_epi8( 0x0, 0x0, 0x0, 0x0, 0x2d, 0x2d, 0x2d, 0x2d, 0x5a, 0x5a, 0x5a, 0x5a, 0x77, 0x77, 0x77, 0x77 );
	const __m128i mul_x4_upper_89ab = _mm_setr_epi8( 0x0, 0x0, 0x0, 0x0, 0x1, 0x1, 0x1, 0x1, 0x2, 0x2, 0x2, 0x2, 0x3, 0x3, 0x3, 0x3 );
	const __m128i mul_x4_upper_4567 = _mm_setr_epi8( 0x0, 0x4, 0x8, 0xc, 0x10, 0x14, 0x18, 0x1c, 0x20, 0x24, 0x28, 0x2c, 0x30, 0x34, 0x38, 0x3c );
	const __m128i mul_x4_upper_0123 = _mm_setr_epi8( 0x0, 0x40, 0x80, 0xc0, 0x0, 0x40, 0x80, 0xc0, 0x0, 0x40, 0x80, 0xc0, 0x0, 0x40, 0x80, 0xc0 );
	
	const __m128i mul_x6_lower_cdef = _mm_setr_epi8( 0x0, 0x6, 0xc, 0xa, 0x18, 0x1e, 0x14, 0x12, 0x30, 0x36, 0x3c, 0x3a, 0x28, 0x2e, 0x24, 0x22 );
	const __m128i mul_x6_lower_89ab = _mm_setr_epi8( 0x0, 0x60, 0xc0, 0xa0, 0x80, 0xe0, 0x40, 0x20, 0x0, 0x60, 0xc0, 0xa0, 0x80, 0xe0, 0x40, 0x20 );
	const __m128i mul_x6_lower_0123 = _mm_setr_epi8( 0x0, 0x0, 0x0, 0x0, 0x2d, 0x2d, 0x2d, 0x2d, 0x77, 0x77, 0x77, 0x77, 0x5a, 0x5a, 0x5a, 0x5a );
	const __m128i mul_x6_upper_89ab = _mm_setr_epi8( 0x0, 0x0, 0x0, 0x0, 0x1, 0x1, 0x1, 0x1, 0x3, 0x3, 0x3, 0x3, 0x2, 0x2, 0x2, 0x2 );
	const __m128i mul_x6_upper_4567 = _mm_setr_epi8( 0x0, 0x6, 0xc, 0xa, 0x18, 0x1e, 0x14, 0x12, 0x30, 0x36, 0x3c, 0x3a, 0x28, 0x2e, 0x24, 0x22 );
	const __m128i mul_x6_upper_0123 = _mm_setr_epi8( 0x0, 0x60, 0xc0, 0xa0, 0x80, 0xe0, 0x40, 0x20, 0x0, 0x60, 0xc0, 0xa0, 0x80, 0xe0, 0x40, 0x20 );

	__m128i tmp;
	//diagonal of ones
	__m128i ret = in;

	__m128i in_high = _mm_srli_epi16(_mm_and_si128(in, midword_zeroes), 4);
	in = _mm_and_si128(in,shuffle_no_zeroing_mask);

	__m128i in_0123 = _mm_and_si128(in_high,shuffle_zeroing_odd_mask);
	__m128i in_4567 = _mm_and_si128(in,shuffle_zeroing_odd_mask);
	__m128i in_89ab = _mm_and_si128(in_high,shuffle_zeroing_even_mask);
	__m128i in_cdef = _mm_and_si128(in,shuffle_zeroing_even_mask);


	//times two
	tmp = shuffle_mult_gf16(in_0123, in_4567, in_89ab, in_cdef, mul_x2_lower_0123, mul_x2_upper_0123, mul_x2_upper_4567, mul_x2_lower_89ab, mul_x2_upper_89ab, mul_x2_lower_cdef);
	ret = _mm_xor_si128(ret, _mm_shuffle_epi8(tmp, shuffle_x2));

	//times four
	tmp = shuffle_mult_gf16(in_0123, in_4567, in_89ab, in_cdef, mul_x4_lower_0123, mul_x4_upper_0123, mul_x4_upper_4567, mul_x4_lower_89ab, mul_x4_upper_89ab, mul_x4_lower_cdef);
	ret = _mm_xor_si128(ret, _mm_shuffle_epi8(tmp, shuffle_x4));

	//times six
	tmp = shuffle_mult_gf16(in_0123, in_4567, in_89ab, in_cdef, mul_x6_lower_0123, mul_x6_upper_0123, mul_x6_upper_4567, mul_x6_lower_89ab, mul_x6_upper_89ab, mul_x6_lower_cdef);
	ret = _mm_xor_si128(ret, _mm_shuffle_epi8(tmp, shuffle_x6));

	return ret;
}

//input in contains 4 vectors to mutliply
//return ret contains 4 vectors multiplied
//multiply using two shuffle multiplications
static inline __m128i anubis_mult_4_shuffle(__m128i in){

	const __m128i midword_zeroes = _mm_setr_epi8(0xf0,0xf0,0xf0,0xf0,0xf0,0xf0,0xf0,0xf0,0xf0,0xf0,0xf0,0xf0,0xf0,0xf0,0xf0,0xf0);
	const __m128i shuffle_no_zeroing_mask = _mm_setr_epi8(0x0f,0x0f,0x0f,0x0f,0x0f,0x0f,0x0f,0x0f,0x0f,0x0f,0x0f,0x0f,0x0f,0x0f,0x0f,0x0f);

	const __m128i shuffle_x2 = _mm_setr_epi8(0x1, 0x0, 0x3, 0x2, 0x5, 0x4, 0x7, 0x6, 0x9, 0x8, 0xb, 0xa, 0xd, 0xc, 0xf, 0xe);
	const __m128i shuffle_x4 = _mm_setr_epi8(0x2, 0x3, 0x0, 0x1, 0x6, 0x7, 0x4, 0x5, 0xa, 0xb, 0x8, 0x9, 0xe, 0xf, 0xc, 0xd);
	const __m128i shuffle_x6 = _mm_setr_epi8(0x3, 0x2, 0x1, 0x0, 0x7, 0x6, 0x5, 0x4, 0xb, 0xa, 0x9, 0x8, 0xf, 0xe, 0xd, 0xc);

	const __m128i mul_x2_low = _mm_setr_epi8( 0x0, 0x2, 0x4, 0x6, 0x8, 0xa, 0xc, 0xe, 0x10, 0x12, 0x14, 0x16, 0x18, 0x1a, 0x1c, 0x1e );
	const __m128i mul_x4_low = _mm_setr_epi8( 0x0, 0x4, 0x8, 0xc, 0x10, 0x14, 0x18, 0x1c, 0x20, 0x24, 0x28, 0x2c, 0x30, 0x34, 0x38, 0x3c );
	const __m128i mul_x6_low = _mm_setr_epi8( 0x0, 0x6, 0xc, 0xa, 0x18, 0x1e, 0x14, 0x12, 0x30, 0x36, 0x3c, 0x3a, 0x28, 0x2e, 0x24, 0x22 );
	
	const __m128i mul_x2_high = _mm_setr_epi8( 0x0, 0x20, 0x40, 0x60, 0x80, 0xa0, 0xc0, 0xe0, 0x1d, 0x3d, 0x5d, 0x7d, 0x9d, 0xbd, 0xdd, 0xfd );
	const __m128i mul_x4_high = _mm_setr_epi8( 0x0, 0x40, 0x80, 0xc0, 0x1d, 0x5d, 0x9d, 0xdd, 0x3a, 0x7a, 0xba, 0xfa, 0x27, 0x67, 0xa7, 0xe7 );
	const __m128i mul_x6_high = _mm_setr_epi8( 0x0, 0x60, 0xc0, 0xa0, 0x9d, 0xfd, 0x5d, 0x3d, 0x27, 0x47, 0xe7, 0x87, 0xba, 0xda, 0x7a, 0x1a );


	__m128i tmp;
	//diagonal of ones
	__m128i ret = in;
	//bytes shifted right by 4, zeroes inserted due to 16bit integer shifts
	__m128i in_high = _mm_srli_epi16(_mm_and_si128(in, midword_zeroes), 4);
	//mask to prevent shuffle zeroing based on highest order bit, ANDed to low order multiplication
	in = _mm_and_si128(in,shuffle_no_zeroing_mask);

	//times two
	tmp = _mm_shuffle_epi8(mul_x2_low,in);
	tmp = _mm_xor_si128(tmp, _mm_shuffle_epi8(mul_x2_high,in_high));
	ret = _mm_xor_si128(ret, _mm_shuffle_epi8(tmp, shuffle_x2));

	//times four
	tmp = _mm_shuffle_epi8(mul_x4_low,in);
	tmp = _mm_xor_si128(tmp, _mm_shuffle_epi8(mul_x4_high,in_high));
	ret = _mm_xor_si128(ret, _mm_shuffle_epi8(tmp, shuffle_x4));

	//times six
	tmp = _mm_shuffle_epi8(mul_x6_low,in);
	tmp = _mm_xor_si128(tmp, _mm_shuffle_epi8(mul_x6_high,in_high));
	ret = _mm_xor_si128(ret, _mm_shuffle_epi8(tmp, shuffle_x6));

	return ret;
}

#endif
